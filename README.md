# 自動シャッタープロジェクト (auto-shutter)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## 概要

このプロジェクトは、カメラと画像認識技術を活用して、特定の条件が満たされたときに自動で写真を撮影するアプリケーションを開発することを目的としています。

## ✨ 主な機能（予定）

*   **リアルタイム映像表示:** PCに接続されたカメラからの映像をリアルタイムで表示します。
*   **顔・物体検出:** 映像の中から顔や特定の物体を検出します。
*   **自動撮影トリガー:** 以下の条件などをトリガーとして、自動でシャッターを切ります。
    *   動きが止まって準備ができたとき
    *   設定した物体が特定の位置に来たとき
*   **QRコード:**　写真をQRコードを用いてダウンロードできるようにする

## 📚 開発環境のセットアップ

このプロジェクトには、開発環境を簡単に構築するための自動セットアップスクリプトが含まれています。

### 必要なもの

*   **Python** (3.8以上を推奨)
*   **Git**

### セットアップ手順

1.  **リポジトリをクローンします。**
    ```bash
    git clone https://github.com/rikutoyamada01/auto-shutter.git
    cd auto-shutter
    ```

2.  **セットアップスクリプトを実行します。**
    以下のコマンドを実行するだけで、仮想環境の作成と必要なライブラリのインストールが自動的に行われます。
    ```bash
    python setup.py
    ```
    スクリプトが完了すると、`.venv` という名前の仮想環境が作成されます。

    <details>
    <summary>💡 <strong>VSCodeを利用している場合</strong></summary>
    
    このセットアップスクリプトは、VSCodeの設定ファイル(`.vscode/settings.json`)を自動で作成・更新し、Pythonインタープリターのパスを仮想環境(`.venv`)に設定します。
    
    スクリプト実行後、VSCodeのウィンドウをリロード（`Ctrl+Shift+P`で`Developer: Reload Window`を選択）すると、自動的に仮想環境が認識され、すぐに開発を始めることができます。
    </details>


### インストールされるライブラリ

このプロジェクトでは、主に以下のライブラリを使用します。(`requirements.txt`参照)

*   **OpenCV (`opencv-python`):** カメラの制御、映像処理、画像認識のコア機能に使用します。
*   **NumPy (`numpy`):** 画像データなどを効率的に扱うための数値計算ライブラリです。

## 🚀 使い方

環境構築後、以下のコマンドを実行するだけでアプリケーションが起動します。
このスクリプトは、自動的に仮想環境を有効化してメインプログラム(`src/main.py`)を実行します。

```bash
python run.py
```

## 🧪 テスト

プロジェクトのテストは、以下のコマンドで実行できます。

```bash
python test.py
```

このスクリプトは、テストフレームワークである`pytest`が仮想環境にインストールされているかを確認し、なければ自動でインストールしてからテストを実行します。

テストコードは、`tests`ディレクトリ（今後作成予定）に、`test_*.py`という命名規則でファイルを作成してください。

## ⚙️ 設計思想

本プロジェクトの中核機能に関する設計思想を記述します。

### 自動ズーム機能（距離調整）

#### 目的
カメラに映る対象物との距離を自動で調整し、フレーム内に収めることを目的とします。

#### 判断の仕組み
カメラの前後移動は、画像処理によって得られる物体が画面横端にはみ出ていないかを判断することで行います。


1.  **はみ出しているの定義**
    背景差分によって検出された物体の輪郭（contour）が背景の横端の余白を残している

2.  **目標余白の決定**
    あらかじめ「このくらいの大きさで表示したい」という**目標余白** をピクセル単位で設定します。

3.  **比較と判断ロジック**
    フレームごとに検出される物体の矩形を比較します。
    *   **現在の距離 < 目標余白:** 物体が近いと判断し、モーターに**「後退」**を指示します。
    *   **現在の距離 > 目標余白:** 物体が遠いと判断し、モーターに**「前進」**を指示します。
    *   **目標の許容範囲内:** ちょうど良い距離と判断し、モーターを**「停止」**させます。

#### 実装ステップ
1.  **ロジックの実装:** まず、モーターを接続する前に、上記の判断ロジックを実装します。コンソールに「前進」「後退」「停止」といった判断結果を`print`文で出力し、意図通りに動作することを確認します。
2.  **ハードウェア連携:** ロジックの正しさが確認できた後、その判断結果に応じてRaspberry PiのGPIOを制御し、実際にモーターを動かす処理を実装します。

## 🤝 コントリビューション（開発ルール）

円滑に共同開発を進めるため、以下のルールを設けます。

*   **ブランチ戦略:**
    *   `main`: 本番用の安定バージョンを配置します。直接のコミットは禁止です。
    *   `develop`: 開発のベースとなるブランチです。
    *   `feature/〇〇`: 機能追加や修正を行うためのブランチです。`develop`から作成し、作業完了後は`develop`へのプルリクエストを作成します。
*   **コミットメッセージ:**
    *   どのような変更か分かりやすいメッセージを記述してください。（例: `feat: 〇〇機能を追加`, `fix: 〇〇のバグを修正`）
*   **プルリクエスト:**
    *   `develop`ブランチへのマージは、必ずプルリクエストを通じて行います。
    *   パートナーがレビューを行い、承認されたらマージします。

## 📄 ライセンス

このプロジェクトは **MIT License** のもとで公開されています。
詳細は [LICENSE](LICENSE) ファイルを参照してください。
